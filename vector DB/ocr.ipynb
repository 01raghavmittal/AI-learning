{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientAuthenticationError",
     "evalue": "(401) Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.\nCode: 401\nMessage: Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mClientAuthenticationError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\learning\\AI-learning\\vector DB\\ocr.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/learning/AI-learning/vector%20DB/ocr.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     image_bytes \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/learning/AI-learning/vector%20DB/ocr.ipynb#W0sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# 4) Analyze - pass bytes directly, choose visual features you need\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/learning/AI-learning/vector%20DB/ocr.ipynb#W0sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m result \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49manalyze(image_data\u001b[39m=\u001b[39;49mimage_bytes,visual_features\u001b[39m=\u001b[39;49m[VisualFeatures\u001b[39m.\u001b[39;49mREAD],language\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/learning/AI-learning/vector%20DB/ocr.ipynb#W0sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# 5) Print caption (if available)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/learning/AI-learning/vector%20DB/ocr.ipynb#W0sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mcaption:\n",
      "File \u001b[1;32mc:\\learning\\AI-learning\\ocr_env\\lib\\site-packages\\azure\\core\\tracing\\decorator.py:119\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39m# If tracing is disabled globally and user didn't explicitly enable it, don't trace.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m user_enabled \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m (\u001b[39mnot\u001b[39;00m tracing_enabled \u001b[39mand\u001b[39;00m user_enabled \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    121\u001b[0m \u001b[39m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[39mif\u001b[39;00m merge_span \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[1;32mc:\\learning\\AI-learning\\ocr_env\\lib\\site-packages\\azure\\ai\\vision\\imageanalysis\\_patch.py:141\u001b[0m, in \u001b[0;36mImageAnalysisClient.analyze\u001b[1;34m(self, image_data, visual_features, language, gender_neutral_caption, smart_crops_aspect_ratios, model_version, **kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Performs a single Image Analysis operation.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[39m:param image_data: A buffer containing the whole image to be analyzed.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39m:raises ~azure.core.exceptions.HttpResponseError:\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    139\u001b[0m visual_features_impl: List[Union[\u001b[39mstr\u001b[39m, _models\u001b[39m.\u001b[39mVisualFeatures]] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(visual_features)\n\u001b[1;32m--> 141\u001b[0m \u001b[39mreturn\u001b[39;00m ImageAnalysisClientOperationsMixin\u001b[39m.\u001b[39m_analyze_from_image_data(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    143\u001b[0m     image_data\u001b[39m=\u001b[39mimage_data,\n\u001b[0;32m    144\u001b[0m     visual_features\u001b[39m=\u001b[39mvisual_features_impl,\n\u001b[0;32m    145\u001b[0m     language\u001b[39m=\u001b[39mlanguage,\n\u001b[0;32m    146\u001b[0m     gender_neutral_caption\u001b[39m=\u001b[39mgender_neutral_caption,\n\u001b[0;32m    147\u001b[0m     smart_crops_aspect_ratios\u001b[39m=\u001b[39msmart_crops_aspect_ratios,\n\u001b[0;32m    148\u001b[0m     model_version\u001b[39m=\u001b[39mmodel_version,\n\u001b[0;32m    149\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    150\u001b[0m )\n",
      "File \u001b[1;32mc:\\learning\\AI-learning\\ocr_env\\lib\\site-packages\\azure\\core\\tracing\\decorator.py:119\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39m# If tracing is disabled globally and user didn't explicitly enable it, don't trace.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m user_enabled \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m (\u001b[39mnot\u001b[39;00m tracing_enabled \u001b[39mand\u001b[39;00m user_enabled \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    121\u001b[0m \u001b[39m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[39mif\u001b[39;00m merge_span \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[1;32mc:\\learning\\AI-learning\\ocr_env\\lib\\site-packages\\azure\\ai\\vision\\imageanalysis\\_operations\\_operations.py:333\u001b[0m, in \u001b[0;36mImageAnalysisClientOperationsMixin._analyze_from_image_data\u001b[1;34m(self, image_data, visual_features, language, gender_neutral_caption, smart_crops_aspect_ratios, model_version, **kwargs)\u001b[0m\n\u001b[0;32m    331\u001b[0m     \u001b[39mif\u001b[39;00m _stream:\n\u001b[0;32m    332\u001b[0m         response\u001b[39m.\u001b[39mread()  \u001b[39m# Load the body in memory and close the socket\u001b[39;00m\n\u001b[1;32m--> 333\u001b[0m     map_error(status_code\u001b[39m=\u001b[39;49mresponse\u001b[39m.\u001b[39;49mstatus_code, response\u001b[39m=\u001b[39;49mresponse, error_map\u001b[39m=\u001b[39;49merror_map)\n\u001b[0;32m    334\u001b[0m     \u001b[39mraise\u001b[39;00m HttpResponseError(response\u001b[39m=\u001b[39mresponse)\n\u001b[0;32m    336\u001b[0m \u001b[39mif\u001b[39;00m _stream:\n",
      "File \u001b[1;32mc:\\learning\\AI-learning\\ocr_env\\lib\\site-packages\\azure\\core\\exceptions.py:163\u001b[0m, in \u001b[0;36mmap_error\u001b[1;34m(status_code, response, error_map)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    162\u001b[0m error \u001b[39m=\u001b[39m error_type(response\u001b[39m=\u001b[39mresponse)\n\u001b[1;32m--> 163\u001b[0m \u001b[39mraise\u001b[39;00m error\n",
      "\u001b[1;31mClientAuthenticationError\u001b[0m: (401) Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.\nCode: 401\nMessage: Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource."
     ]
    }
   ],
   "source": [
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "\n",
    "ENDPOINT = os.getenv(\"VISION_ENDPOINT\")\n",
    "SUBSCRIPTION_KEY = os.getenv(\"VISION_KEY\")\n",
    "\n",
    "# 2) Create the client\n",
    "client = ImageAnalysisClient(\n",
    "    endpoint = ENDPOINT,\n",
    "    credential =AzureKeyCredential(SUBSCRIPTION_KEY))\n",
    "\n",
    "# 3) Read a local image as bytes\n",
    "with open(\"presentation.png\", \"rb\") as f:\n",
    "    image_bytes = f.read()\n",
    "\n",
    "# 4) Analyze - pass bytes directly, choose visual features you need\n",
    "result = client.analyze(image_data=image_bytes,visual_features=[VisualFeatures.READ],language=\"en\")\n",
    "\n",
    "# 5) Print caption (if available)\n",
    "if result.caption:\n",
    "    print(\"Caption:\", result.caption.text)\n",
    "\n",
    "# 6) Print OCR text (if available)\n",
    "if result.read:\n",
    "    print(\"\\nDetected text:\")\n",
    "    for block in result.read.blocks:\n",
    "        for line in block.lines:\n",
    "            print(line.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ocr_env)",
   "language": "python",
   "name": "ocr_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
